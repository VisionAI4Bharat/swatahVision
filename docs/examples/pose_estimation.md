# Pose Estimation using MoveNet (ONNX)

This project demonstrates **human pose estimation** using a MoveNet ONNX model.
It supports image and video inference and provides structured pose outputs (keypoints + confidence) along with visualization.

---

## ğŸš€ Features

* MoveNet ONNX inference (CPU)
* Image and video pose estimation
* Keypoints parsing (17 COCO joints)
* Skeleton visualization
* Clean class-based architecture
* Compatible with SwatahVision style parsers

---

## ğŸ“ Project Structure

```
pose_estimation/
â”‚
â”œâ”€â”€ Movenet.onnx                # Pose estimation model
â”œâ”€â”€ movenet_pose.py             # Inference class (video / image)
â”œâ”€â”€ pose.py                     # Pose parser (from_movenet)
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ sample.jpg              # Example input image
â””â”€â”€ README.md
```

---

## ğŸ“¦ Requirements

Install dependencies:

```bash
pip install onnxruntime opencv-python numpy
```

(Optional if using SwatahVision)

```bash
pip install swatahvision
```

---

## â¬‡ï¸ Download the MoveNet ONNX Model

### Option 1 â€” Download pre-converted ONNX (recommended)

1. Download MoveNet ONNX model from a model hub (example sources):

   * ONNX model zoo mirrors
   * TensorFlow â†’ ONNX community conversions
   * Your internal model storage

2. Rename the file to:

```
Movenet.onnx
```

3. Place it inside:

```
pose_estimation/
```

---

### Option 2 â€” Convert MoveNet to ONNX yourself

If you have TensorFlow MoveNet:

Install:

```bash
pip install tf2onnx tensorflow
```

Convert:

```bash
python -m tf2onnx.convert \
--saved-model movenet_saved_model \
--output Movenet.onnx \
--opset 13
```

Move the generated file into the project folder.

---

## â–¶ï¸ Run Pose Estimation (Image)

Example:

```python
import swatahvision as sv
from pose import Pose

model = sv.Model(
    model="pose_estimation/Movenet.onnx",
    engine=sv.Engine.ONNX,
    hardware=sv.Hardware.CPU
)

image = sv.Image.load_from_file("pose_estimation/assets/sample.jpg")

outs = model(image)

pose = Pose.from_movenet(outs)

print(pose.keypoints)
print(pose.confidence)
```

---

## â–¶ï¸ Run Pose Estimation (Video)

```bash
python movenet_pose.py
```

Press **q** to exit.

---

## ğŸ§  Output Format

MoveNet returns 17 keypoints:

```
[x, y] coordinates
confidence score
```

Keypoints follow COCO order:

* nose
* eyes
* ears
* shoulders
* elbows
* wrists
* hips
* knees
* ankles

---

## ğŸ› ï¸ How it works

Pipeline:

1. Load ONNX model
2. Preprocess frame (resize â†’ normalize)
3. Run ONNX inference
4. Parse keypoints via `Pose.from_movenet`
5. Draw skeleton

---

## âœ… Supported Tasks

* Pose estimation
* Real-time video pose
* Visualization
* Framework integration (SwatahVision)

---

## ğŸ“Œ Notes

* Model expects **192Ã—192 input**
* Works on CPU
* Confidence threshold can be adjusted
* Compatible with other pose ONNX models with similar output

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a branch
3. Make changes
4. Open Pull Request

---

## ğŸ“„ License

Add your license here (MIT recommended).
